# -*- coding: utf-8 -*-
"""sydney_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqJuY-ykqIwVXh3pMpYIxrzHZim2usdI

## initializing
"""

pip install requests python-dotenv

import os

# Set the GitHub token as an environment variable
os.environ['GITHUB_TOKEN'] = "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"

!pip install requests python-dotenv

import os
print(os.getenv("GITHUB_TOKEN"))

import requests
import os

HEADERS = {'Authorization': f'token {os.getenv("GITHUB_TOKEN")}'}
response = requests.get("https://api.github.com/user", headers=HEADERS)

print("Status Code:", response.status_code)
print("Response JSON:", response.json())

import requests
import time

def get_sydney_users():
    users = []
    url = 'https://api.github.com/search/users'
    query = 'location:Sydney followers:>100'
    params = {'q': query, 'per_page': 100}
    page = 1
    unique_logins = set()
    total_count = 371  # Expected total number of users

    while len(users) < total_count:
        params['page'] = page
        response = requests.get(url, headers=HEADERS, params=params)

        # Check rate limit
        if response.status_code == 403:  # Rate limit exceeded
            print("Rate limit exceeded. Pausing for 1 minute.")
            time.sleep(60)  # Wait for 1 minute and retry
            continue

        if response.status_code == 422:
            print("Error 422: Unprocessable Entity")
            print("Likely cause: Pagination or data error.")
            break

        if response.status_code != 200:
            print(f"Unexpected Error: {response.status_code}")
            break

        # Process items on current page
        data = response.json().get('items', [])

        if not data:  # If no data is returned, break the loop
            break

        # Add unique users to the list
        for user in data:
            if user['login'] not in unique_logins:
                user_details = get_user_details(user['login'])
                if user_details and 'Sydney' in (user_details.get('location') or '') and user_details['followers'] >= 100:
                    users.append(user_details)
                    unique_logins.add(user['login'])

        page += 1  # Move to the next page

    return users[:total_count]

get_sydney_users()

len(get_sydney_users())

# Check rate limit before proceeding
rate_limit = requests.get("https://api.github.com/rate_limit", headers=HEADERS)
rate_limit_data = rate_limit.json()
print("Remaining requests:", rate_limit_data['rate']['remaining'])

"""
## Collect Data step"""

import csv
import requests

def collect_data():
    # Define headers for GitHub API requests (replace 'YOUR_GITHUB_TOKEN' with your actual token)
    HEADERS = {'Authorization': 'token YOUR_GITHUB_TOKEN'}

    # Define fieldnames for users.csv and repositories.csv
    user_fieldnames = [
        'login', 'name', 'company', 'location', 'email', 'hireable',
        'bio', 'public_repos', 'followers', 'following', 'created_at'
    ]
    repo_fieldnames = [
        'login', 'full_name', 'created_at', 'stargazers_count', 'watchers_count',
        'language', 'has_projects', 'has_wiki', 'license_name'
    ]

    # Open users.csv and repositories.csv for writing
    with open('users.csv', 'w', newline='', encoding='utf-8') as users_file, \
         open('repositories.csv', 'w', newline='', encoding='utf-8') as repos_file:

        # Set up CSV writers
        users_writer = csv.DictWriter(users_file, fieldnames=user_fieldnames)
        repos_writer = csv.DictWriter(repos_file, fieldnames=repo_fieldnames)

        # Write headers for both files
        users_writer.writeheader()
        repos_writer.writeheader()

        # Call get_sydney_users to retrieve user data
        users = get_sydney_users()
        unique_logins = set()  # Track unique logins to avoid duplicates

        # Process each user in the list
        for user in users:
            login = user['login']
            if login not in unique_logins:
                # Retrieve detailed user data
                user_details = get_user_details(login)

                if user_details:
                    # Extract relevant user fields and write to users.csv
                    user_data = {
                        'login': user_details['login'],
                        'name': user_details.get('name', ''),
                        'company': (user_details.get('company') or '').strip().upper(),
                        'location': user_details.get('location', ''),
                        'email': user_details.get('email', ''),
                        'hireable': user_details.get('hireable', ''),
                        'bio': user_details.get('bio', ''),
                        'public_repos': user_details['public_repos'],
                        'followers': user_details['followers'],
                        'following': user_details['following'],
                        'created_at': user_details['created_at']
                    }
                    users_writer.writerow(user_data)
                    unique_logins.add(login)  # Add to set to avoid duplicates

                    # Retrieve and process repositories for the user
                    repos = get_user_repos(login)
                    for repo in repos:
                        # Extract relevant repository fields and write to repositories.csv
                        repo_data = {
                            'login': login,
                            'full_name': repo['full_name'],
                            'created_at': repo['created_at'],
                            'stargazers_count': repo['stargazers_count'],
                            'watchers_count': repo['watchers_count'],
                            'language': repo['language'] or '',
                            'has_projects': repo['has_projects'],
                            'has_wiki': repo['has_wiki'],
                            'license_name': repo['license']['key'] if repo['license'] else ''
                        }
                        repos_writer.writerow(repo_data)

    print("Data collection complete. Saved to users.csv and repositories.csv successfully.")

collect_data()

from google.colab import files

# Download users.csv and repositories.csv
files.download('users.csv')
files.download('repositories.csv')

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('users.csv')

# Display the first few rows to confirm it loaded correctly
users_df.head()

users_df.shape

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('users.csv')

# Check for duplicates in the 'login' column
duplicates = users_df[users_df.duplicated(subset='login', keep=False)]
print("Duplicate Entries in users.csv:")
duplicates

duplicates.shape

"""## Question1"""

import pandas as pd

# Load the data
users_df = pd.read_csv('users.csv')

# Sort by followers and get top 5
top_users = users_df.sort_values(by='followers', ascending=False).head(5)

# Extract logins
top_logins = top_users['login'].tolist()
result = ', '.join(top_logins)

print(result)

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Sort by followers in descending order and select the top 5 users
top_users = users_df.sort_values(by='followers', ascending=False).head(5)

# Create a comma-separated list of their login names
top_users_logins = ", ".join(top_users['login'])

print("Top 5 users in Sydney with the highest number of followers:")
print(top_users_logins)

"""## Question 2"""

import pandas as pd

# Load the data
users_df = pd.read_csv('users.csv')

# Convert created_at to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Sort by created_at and get the earliest 5 users
earliest_users = users_df.sort_values(by='created_at').head(5)

# Extract logins
earliest_logins = earliest_users['login'].tolist()
result = ', '.join(earliest_logins)

print(result)

# Convert the created_at column to datetime format if it's not already
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Sort users by created_at in ascending order and get the top 5
earliest_users = users_df[['login', 'created_at']].sort_values(by='created_at', ascending=True).head(5)

# Join the login names as a comma-separated string
earliest_users_login = ", ".join(earliest_users['login'])

print("5 earliest registered GitHub users in Sydney:")
print(earliest_users_login)

"""## Question 3"""

import pandas as pd

# Load the data
repositories_df = pd.read_csv('repositories.csv')

# Filter out missing license names
repositories_df = repositories_df[repositories_df['license_name'].notna()]

# Count occurrences of each license
license_counts = repositories_df['license_name'].value_counts()

# Get the top 3 licenses
top_licenses = license_counts.head(3).index.tolist()

# Join the license names in order
result = ', '.join(top_licenses)

print(result)

import pandas as pd

# Load the repositories.csv file
repositories_df = pd.read_csv('repositories.csv')

# Display the first few rows to confirm it loaded correctly
print(repositories_df.head())

# Filter out rows with missing license_name
licensed_repos = repositories_df[repositories_df['license_name'].notna()]

# Count the occurrences of each license
license_counts = licensed_repos['license_name'].value_counts()

# Get the top 3 most common licenses
top_licenses = license_counts.head(3).index

# Join the top 3 license names as a comma-separated string
top_licenses_str = ", ".join(top_licenses)

print("3 most popular licenses among these users:")
print(top_licenses_str)

import pandas as pd

# Load the data
users_df = pd.read_csv('users.csv')

# Count occurrences of each company
company_counts = users_df['company'].value_counts()

# Get the company with the highest count
most_common_company = company_counts.idxmax()
most_common_count = company_counts.max()

print(f"The majority of developers work at: {most_common_company} with {most_common_count} developers.")

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Clean up the company names
def clean_company_name(company):
    if pd.isnull(company):  # Handle missing values
        return ''
    company = company.strip().upper()  # Trim whitespace and convert to uppercase
    if company.startswith('@'):  # Remove leading '@' if present
        company = company[1:]
    return company

# Apply the cleaning function to the company column
users_df['company'] = users_df['company'].apply(clean_company_name)

# Find the most common company among users
most_common_company = users_df['company'].mode()[0]


print(most_common_company)

clean_company_name(company)

import pandas as pd

# Load the repositories.csv file into a DataFrame
repos_df = pd.read_csv('repositories.csv')

# Drop rows with missing language values to focus on non-empty entries
repos_df = repos_df.dropna(subset=['language'])

# Find the most common programming language
most_common_language = repos_df['language'].mode()[0]

print("Most popular programming language among these users:")
print(most_common_language)

import pandas as pd

# Load the users.csv and repositories.csv files into DataFrames
users_df = pd.read_csv('users.csv')
repos_df = pd.read_csv('repositories.csv')

# Convert the created_at column in users_df to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Step 1: Filter users who joined after 2020
recent_users = users_df[users_df['created_at'] > '2020-12-31']

# Step 2: Get the logins of these users
recent_user_logins = recent_users['login'].unique()

# Step 3: Filter repositories belonging to these users
recent_repos = repos_df[repos_df['login'].isin(recent_user_logins)]

# Step 4: Drop rows with missing language values
recent_repos = recent_repos.dropna(subset=['language'])

# Step 5: Count occurrences of each programming language
language_counts = recent_repos['language'].value_counts()

# Step 6: Get the second most popular language, if it exists
if len(language_counts) > 1:
    second_most_popular_language = language_counts.index[1]
else:
    second_most_popular_language = None

print("Second most popular programming language among users who joined after 2020:")
print(second_most_popular_language)

import pandas as pd

# Load the data
users_df = pd.read_csv('users.csv')
repositories_df = pd.read_csv('repositories.csv')

# Convert created_at to datetime and filter users who joined after 2020
users_df['created_at'] = pd.to_datetime(users_df['created_at'])
recent_users = users_df[users_df['created_at'] > '2020-01-01']

# Get the logins of recent users
recent_user_logins = recent_users['login'].tolist()

# Filter repositories by these users
recent_repositories = repositories_df[repositories_df['login'].isin(recent_user_logins)]

# Count occurrences of each programming language
language_counts = recent_repositories['language'].value_counts()

# Get the second most popular programming language
second_most_popular_language = language_counts.nlargest(2).index[1]
second_most_popular_count = language_counts.nlargest(2).values[1]

print(f"The second most popular programming language among users who joined after 2020 is: {second_most_popular_language} with {second_most_popular_count} repositories.")

import pandas as pd

# Load the repositories.csv file into a DataFrame
repos_df = pd.read_csv('repositories.csv')

# Drop rows where 'language' or 'stargazers_count' is missing to ensure valid data
repos_df = repos_df.dropna(subset=['language', 'stargazers_count'])

# Group by 'language' and calculate the average 'stargazers_count' for each language
language_avg_stars = repos_df.groupby('language')['stargazers_count'].mean()

# Find the language with the highest average stars
most_popular_language = language_avg_stars.idxmax()
highest_avg_stars = language_avg_stars.max()

print("Language with the highest average number of stars per repository:")
print(f"{most_popular_language} with an average of {highest_avg_stars:.2f} stars")

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Calculate leader_strength as followers / (1 + following)
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Sort by leader_strength in descending order and select the top 5 unique users
top_leaders = users_df.sort_values(by='leader_strength', ascending=False).head(5)

# Generate a comma-separated list of the top 5 user logins
top_leader_logins = ", ".join(top_leaders['login'])

print("Top 5 users in terms of leader_strength:")
print(top_leader_logins)

#Q8
import pandas as pd

# Load the data
users_df = pd.read_csv('users.csv')

# Calculate leader_strength
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Sort by leader_strength and get the top 5
top_leaders = users_df.sort_values(by='leader_strength', ascending=False).head(5)

# Extract logins
top_logins = top_leaders['login'].tolist()
result = ', '.join(top_logins)

print(result)

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Calculate the correlation between 'followers' and 'public_repos'
correlation = users_df['followers'].corr(users_df['public_repos'])

# Print the correlation rounded to 3 decimal places
print("Correlation between followers and repos:", round(correlation, 3))

import pandas as pd
import statsmodels.api as sm

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Define the independent variable (public_repos) and dependent variable (followers)
X = users_df['public_repos']
y = users_df['followers']

# Add a constant to the independent variable for the intercept term
X = sm.add_constant(X)

# Perform the linear regression
model = sm.OLS(y, X).fit()

# Get the slope (coefficient of public_repos)
slope = model.params['public_repos']

# Print the slope rounded to 3 decimal places
print("Regression slope of followers on repos:", round(slope, 3))

import pandas as pd

# Load the repositories.csv file into a DataFrame
repos_df = pd.read_csv('repositories.csv')

# Ensure 'has_projects' and 'has_wiki' columns are properly converted to integer (1 for True, 0 for False)
repos_df['has_projects'] = repos_df['has_projects'].astype(int)
repos_df['has_wiki'] = repos_df['has_wiki'].astype(int)

# Recalculate the correlation between 'has_projects' and 'has_wiki'
correlation = repos_df['has_projects'].corr(repos_df['has_wiki'])

# Print the correlation rounded to 3 decimal places
print("Correlation between projects and wiki enabled:", round(correlation, 3))

import pandas as pd

# Load the users data from the CSV file
users_df = pd.read_csv('users.csv')

# Filter hireable and non-hireable users
hireable_users = users_df[users_df['hireable'] == True]
non_hireable_users = users_df[users_df['hireable'].isna() | (users_df['hireable'] == False)]

# Calculate average following for both groups
average_hireable_following = hireable_users['following'].mean()
average_non_hireable_following = non_hireable_users['following'].mean()

# Calculate the difference
difference = average_hireable_following - average_non_hireable_following

# Print the result rounded to three decimal places
print(f'Difference in average following (hireable - non-hireable): {difference:.3f}')

import pandas as pd

# Load the repositories.csv file into a DataFrame
repos_df = pd.read_csv('repositories.csv')

# Convert 'has_projects' and 'has_wiki' columns to integer values (True=1, False=0)
repos_df['has_projects'] = repos_df['has_projects'].astype(int)
repos_df['has_wiki'] = repos_df['has_wiki'].astype(int)

# Calculate the number of repositories for each combination of has_projects and has_wiki
combination_counts = repos_df.groupby(['has_projects', 'has_wiki']).size().unstack(fill_value=0)

# Display the counts of each combination
print("Combination Counts of Projects and Wiki Enabled:")
print(combination_counts)

# Calculate the proportion of repositories where both features are enabled
both_enabled = combination_counts.loc[1, 1]  # Both projects and wiki enabled
total_repos = len(repos_df)
proportion_both_enabled = both_enabled / total_repos

print("\nProportion of repositories with both projects and wiki enabled:")
print(round(proportion_both_enabled, 3))

# Calculate the Pearson correlation as before
correlation = repos_df['has_projects'].corr(repos_df['has_wiki'])
print("\nCorrelation between projects and wiki enabled:", round(correlation, 3))

import pandas as pd
import statsmodels.api as sm

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Filter out users without a bio
users_with_bio = users_df.dropna(subset=['bio'])

# Calculate word count for each bio
users_with_bio['bio_word_count'] = users_with_bio['bio'].apply(lambda x: len(x.split()))

# Define the independent variable (bio_word_count) and dependent variable (followers)
X = users_with_bio['bio_word_count']
y = users_with_bio['followers']

# Add a constant to the independent variable for the intercept
X = sm.add_constant(X)

# Perform the linear regression
model = sm.OLS(y, X).fit()

# Get the slope (coefficient of bio_word_count)
slope = model.params['bio_word_count']

# Print the slope rounded to 3 decimal places
print("Regression slope of followers on bio word count:", round(slope, 3))

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Calculate the fraction of hireable users with an email, handling potential division by zero
hireable_count = len(users_df[users_df['hireable'] == True])
if hireable_count > 0:
    hireable_with_email = users_df[(users_df['hireable'] == True) & (users_df['email'].notna())]
    fraction_hireable_with_email = len(hireable_with_email) / hireable_count
else:
    fraction_hireable_with_email = 0

# Calculate the fraction of non-hireable users with an email, handling potential division by zero
non_hireable_count = len(users_df[users_df['hireable'] == False])
if non_hireable_count > 0:
    non_hireable_with_email = users_df[(users_df['hireable'] == False) & (users_df['email'].notna())]
    fraction_non_hireable_with_email = len(non_hireable_with_email) / non_hireable_count
else:
    fraction_non_hireable_with_email = 0

# Calculate the difference
difference = fraction_hireable_with_email - fraction_non_hireable_with_email

# Print the result rounded to 3 decimal places
print("Difference in fraction of users with email (hireable - non-hireable):", round(difference, 3))

import pandas as pd

# Load the users.csv file into a DataFrame
users_df = pd.read_csv('users.csv')

# Filter out rows with missing names
users_with_names = users_df.dropna(subset=['name'])

# Extract the last word in each name as the surname
users_with_names['surname'] = users_with_names['name'].str.strip().str.split().str[-1]

# Count the frequency of each surname
surname_counts = users_with_names['surname'].value_counts()

# Get the highest frequency
max_count = surname_counts.iloc[0]

# Find all surnames with the maximum count and sort alphabetically
most_common_surnames = surname_counts[surname_counts == max_count].index.sort_values()

# Convert the result to a comma-separated string
most_common_surnames_str = ", ".join(most_common_surnames)

print("Most common surname(s):")
print(most_common_surnames_str)

import pandas as pd

# Load the users data from the CSV file
users_df = pd.read_csv('users.csv')

# Filter out users without names
valid_users = users_df[users_df['name'].notna()]

# Extract surnames (last word in name)
valid_users['surname'] = valid_users['name'].str.strip().str.split().str[-1]

# Count occurrences of each surname
surname_counts = valid_users['surname'].value_counts()

# Find the most common surname(s)
max_count = surname_counts.max()
most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()

# Sort surnames alphabetically
most_common_surnames.sort()

# Count users with the most common surname
number_of_users = max_count

# Print results
most_common_surnames_str = ', '.join(most_common_surnames)
print(f'Most common surname(s): {most_common_surnames_str}')
print(f'Number of users with the most common surname: {number_of_users}')

